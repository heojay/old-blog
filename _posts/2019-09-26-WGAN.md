---
title: "WGAN 정리"
date: 2019-09-26 08:26:28 -0400
categories: study
use_math: true
---

연세대학교 강효림님의 강의를 듣고 정리한 글입니다.

향후 그림과, WGAN-GP를 추가해서 업데이트 할 예정입니다.



## KL Divergence의 한계


$$
D_{KL}(P\parallel Q)=-\int_x p(x)\ln \frac{q(x)}{p(x)} dx
\\
{{D_{JS}}}(P\parallel Q)={\frac  {1}{2}}D_{KL}(P\parallel \frac{p+q}{2})+{\frac  {1}{2}}D_{KL}(Q\parallel \frac{p+q}{2})
$$


지난 포스팅에서 다뤘던 KL Divergence는 확률 분포간의 거리라고 부르기에는 대칭성이 성립하지 않는다는, 즉 p와 q의 순서가 변화하면 값이 변한다는 한계가 있었다. **JS Divergence**는 그 대칭성을 부여하기 위한 방법이라고 할 수 있다.

그러나 이 JS Divergence에도 한계가 있다.


$$
\min_G\max_DV(D,G) = \mathbb{E}_{x\sim p_{data}(x)}[logD(x)] + \mathbb{E}_{x\sim p_{x}(z)}[1-log(1-D(G(X)))]
$$


여기서 $D^*(x)= \frac {p_r(x)}{p_r(x)+p_g(x)}$를 만족할 때, 즉 Discriminator가 가장 이상적인 값일 때


$$
\min_GV(D^*,G) = 2D_{JS}{(p_r\|p_g)}-2\log2
$$


식으로 바꿔줄 수 있다.

JS Divergence식을 생각하면, 두 분포의 0이 아닌 값이 (supports) 불일치 할 경우 항상 $\log2$의 값을 가진다는 것을 알 수 있다. 그러나 불일치라고 해도 가까스로 불일치 되는 경우가 있고, 완전 동떨어져있는 경우도 있을 것이다. 즉, JS Divergence로는 GAN을 충분히 표현할 수 없다는 뜻이다.



## Wasserstein Distance

상술한 문제를 해결하기 위해 **Wasserstein Distance**가 제안된다.


$$
W(P_r,P_g) = \inf_{\gamma\in\prod(P_r,P_g)}E_{(x,y)\sim \gamma}[\|x-y\|]
$$


우리가 블럭으로 성을 하나 쌓았다고 가정하자, 이 성을 다른 위치로 옮긴다면 비용이 발생할 것이다. 뻘짓을 하면 비용이 많이 발생할 것이고, 체계적으로 옮긴다면 적게 발생할 것이다.

마찬가지로, 확률분포가 있다고 했을 때, 그 분포를 다른 확률분포의 위치로 이동시킨다고 하자. 그 각각의 방법을 '결합확률분포'라고 한다. 이 중 기대값이 가장 작을 때의 거리를 Wasserstein Distance라고 한다.

엄밀히 말해 최소값은 아니다. 최소값이 없을 수도 있기 때문 ($\min$이 항상 있지는 않으니까). 최소값에 가장 가까운 값이라고 하는 것이 더 맞는 표현이다.



### 적용

이 거리를 직접 계산하는 것은 어렵기 때문에 동치 문제인 **Kantorovich-Rubinstein Duality Theorem**을 이용한다. 식은 다음과 같다.


$$
W(P_r,P_g)= \sup_{\|f\|_L\leq1}E_{x\sim p_r}[f(x)]-{x\sim p_g}[f(x)]
$$


여기서 $\|f\|_L$은 Lipschitz condition을 만족시킴을 의미하는데, 이는 기울기의 절대값이 1을 넘지 않는다는 뜻을 말한다. 쉽게 말해 평탄한 함수라는 것이다. $f(x)$는 $f_\theta(x)$, 즉 뉴럴 넷으로 파라미터화 되고, 이는 역전파로 업데이트 할 수 있다.

남은 문제인 Lipschitz condition에 대해서는 clip 하는 것으로 해결한다. 파라미터를 일정 수준으로 제한하여 갑작스러운 변화를 막는 것이다. 논문에서는 *Terrible solution*이라고 자조하지만, 좋은 결과를 낳는다고 한다.



그럼 GAN은 어떤 변화가 생길까? 우선 Discriminator는 Actor-Critic의 Critic 역할로 대체한다.

GAN은 어디까지나 확률의 문제였다. D가 구별하는 확률, 속는 확률. 이제는 확률을 벗어나 점수의 한계가 사라졌으니 이제는 그 차이를 벌리도록 학습한다. 이를 위해 활성함수에서 최대값을 제한하고 있는 sigmoid를 제거하기로 한다.

여기에 더해 언급했듯, Lipschitz condition을 만족시키기 위해 Parameter에 제약을 걸기로 한다.