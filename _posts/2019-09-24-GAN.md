---
title: "GAN, InfoGAN 정리"
date: 2019-09-24 08:26:28 -0400
categories: study
use_math: true

---



## Generative Adversarial Network (GAN)

GAN은 실제 데이터의 분포와 비슷한 분포를 가진 데이터를 생성하는 것이 목표이다.



### 구조

데이터를 생성하는 Generator와 이 데이터를 평가하는 Discriminator가 있다. Generator의 목표는 data의 분포를 알아내는 것. 생성된 data와 가지고 있는 data의 분포가 같다면 구분할 수 없을 것. 한편 Discrminator의 목표는 판별하고자 하는 data가 G에 의해 생성된 data인지, 가지고 있는 data인지 확률을 예측한다.

D는 실수할 확률을 낮추고, G는 그 확률을 높이는 minimax한 관계 속에서 서로의 성능을 향상시키고자 하는 것.

이를 수식으로 정리하면 다음과 같다.


$$
\min_G\max_DV(D,G) = \mathbb{E}_{x\sim p_{data}(x)}[logD(x)] + \mathbb{E}_{x\sim p_{x}(z)}[1-log(1-D(G(X)))]
$$


$\mathbb{E}_{x\sim p_{data}(x)}[logD(x)]$는 실제 데이터를 실제 데이터로 맞출 확률. D는 최대화, G는 최소화 하려고 한다.  $\mathbb{E}_{x\sim p_{x}(z)}[1-log(1-D(G(X)))]$는 $G$가 만들어낸 데이터를 D가 가짜라고 판단할 확률이다. G는 최대화, D는 최소화 하려고 한다.



## InfoGAN (Information Maximizing Generative Adversarial Nets)

GAN은 노이즈의 분포가 일정하지 않다는 한계가 있었다. 따라서 latent variable를 input으로 추가해주어서, 생성된 샘플과 latent variable 사이의 Mutual Information이 높아지도록 하는 것이 목표이다.



### 구조

기존 GAN의 input은 노이즈 ($z$) 하나였다. 여기에 code라는 latent variable ($c$)가 추가된다. 그러나 단순히 $c$를 추가하는 것은 노이즈를 추가하는 것과 같다. 따라서 노이즈와 생성된 샘플 사이의 관계성을 학습할 필요가 있다. 이를 위해 상호정보량을 계산하기 위한 식을 추가한다.


$$
\min_G\max_DV_I(D,G) = V(D,G) - \lambda I(c;G(z,c))
$$


GAN이 풀고자 했던 minimax 문제에 $\lambda I(c;G(z,c))$ 가 추가로 생긴 것을 확인할 수 있다. G는 위 식을 최대화해야 한다. 즉, $c$와 $G(z,c)$가 높은 상호정보량을 갖게 학습한다는 것이다. 다만, 이를 계산하기 어렵기 때문에 계산이 편한 lower bound를 이용하며, 그 식은 다음과 같다.


$$
\min_{G, Q}\max_DV_{InfoGAN}(D,G) = V(D,G) - \lambda L_1(c;G(z,c))
$$


다음은 이를 구현하도록 하자.